import tensorflow_hub as hub
import tensorflow as tf
import pandas as pd
import numpy as np
import seaborn as sns

#Function to compute all embeddings for each sentence:
#Be patient, takes a little while:
def similarity_matrix(merge_list):
    #initialize distance array:
    #initialize embeddings array:
    emb_all = np.zeros([len(merge_list),512])
    #Outer for loop:
    for i in range(0,len(merge_list)):
        #Here is where we run the previously started session, so it is important to run previous step succesfully:
        i_emb = session.run(embedded_text, feed_dict={text_input: [merge_list[i]]})
        emb_all[i,:] = i_emb
    return emb_all


df = pd.read_csv('filteredData.csv',header=None)
input = df.iloc[:, 1:]
labels = df.iloc[:, 0]

g = tf.Graph()
with g.as_default():
  text_input = tf.placeholder(dtype=tf.string, shape=[None])
  embed = hub.Module("https://tfhub.dev/google/universal-sentence-encoder-large/3")
  embedded_text = embed(text_input)
  init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])
g.finalize()


session = tf.Session(graph=g)
session.run(init_op)
features = similarity_matrix(input)

print(features.shape)
features.to_csv("features.csv")
labels.to_csv("labels.csv")